<!DOCTYPE html>
<html>

  	<head>
    	<title>Brett Baribault Bankson</title>
    	<link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">
    	<link href="https://fonts.googleapis.com/css?family=Nanum+Gothic&display=swap" rel="stylesheet">
    	<link href="https://fonts.googleapis.com/css?family=Comfortaa&display=swap" rel="stylesheet">
    	<link href="/static/pages.css" rel="stylesheet" type="text/css">
    	<script src="https://kit.fontawesome.com/8ad481cdeb.js"></script>
	</head>

	<style>

		body {
			font-family: 'Nanum Gothic', sans-serif;
			background-color: white;
			padding:25px;
			max-width:90%;
		}

		#title_div{
			top:60px;
			border:1px solid red;
			font-family: 'Comfortaa', cursive;
			line-height:50px;
			font-size:50px;
			margin-bottom:20px;
		}

		.card {
			display:table;
			border:1px solid blue;
		}

		.description{
			display:table-cell;
			width:50%;
			border:1px solid green;
			padding:20px;
		}

		.question{
			display:table-cell;
			width:50%;
			border:1px solid blue;
			padding:20px;
		}

		.fa-question-circle{
			color: #285E61;
			font-size:4rem;
		}

		.emphasis-span{
			font-family: 'Nanum Gothic Extra-Bold', sans-serif;
		}

	</style>
	
	<body>
		<div class="nav">
			<a href="/" style="margin-right:30px">Home</a>
			<a href="/about" style="margin-right:30px">About</a>
			<a href="/research" style="margin-right:30px">Research</a>
			<a href="/food">Food</a>
		</div>

		<div id="title_div">
			RESEARCH
		</div>

		<div>
			<span style="color: #234E52; font-size:2rem;">The human brain rapidly and effortlessly extracts visual information from our environments on a millisecond-to-millisecond basis.</span>

			<p>
				How does coordinated activity throughout visually responsive areas of the brain compute this input to yield <span class="emphasis-span">high-level information</span> that informs mental states and behavior? There are several research avenues I’m working on to further our understanding of this process, using spatiotemporally-resolved recordings from <i>intracranial electroencephalography (iEEG)</i> to provide sensitive and dynamic portraits of brain activity.
			</p>
		</div>

		<div class="card">
			<div class="description">
				Spatiotemporal dynamics of face perception and natural vision: When we see a face, we encode information from many feature dimensions – identity, age, gender, expression, gaze direction, etc. These features are functionally relevant and shape our interpretation of people in front of us. 
			</div>
			<div class="question">
				<div>
					<span class="question-span">
						<i class="far fa-question-circle"></i> How are different face feature dimensions that form and motion-related aspects encoded in time across ventral and dorsal visual streams?
					</span>
				</div>
				<div>
					<span class="question-span">
						<i class="far fa-question-circle"></i> Do non-face preferring areas represent face information that is independent from canonical face preferring areas, and does this extend to other categories of visual objects?
					</span>
				</div>
				<div>
					<span class="question-span">
						<i class="far fa-question-circle"></i> How does the brain extract identity and affective information from dynamic videos of spontaneous facial expressions?
					</span>
				</div>
			</div>
		</div>

		<div>
			Retinotopy and visual perception: The location of an object in visual space drives neural activity selectively according to position near or far from fixation and along the horizontal and vertical meridians. The time course by which activity in category-selective areas is modulated by retinotopic variation is not well understood, and I’m working to better quantify the relationship between retinotopy and high-level visual processing with the following questions: How does the position of a face in the left or right visual hemifield affect the representation of face identity in ventral temporal face selective areas? When does variation in retinotopic location at a fixed eccentricity affect representational fidelity for individual faces, words, and phase-scrambled stimuli?
		</div>

		<div>
			Numerical cognition and visual perception: Most numerical reasoning that humans engage in relies on symbolic number characters (digits) to represent nonsymbolic magnitude information. It is currently unclear the extent to which magnitude representations are shared across representational formats (digits, words, dot arrays), and furthermore how these representations emerge in time. With collaborators at Pitt and Harvard, I’m asking the following questions: What is the time course by which individual numbers across various representational formats can be classified from whole-head MEG signal, and how do models of low-level visual features and magnitude explain this evoked response? How do number-selective sites in VTC and the inferior parietal sulcus (IPS) interact to assign magnitude information to visually-perceived digits and dot displays?
		</div>

	</body>
</html>
