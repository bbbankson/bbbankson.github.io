<!DOCTYPE html>
<html>

  	<head>
    	<title>Brett Baribault Bankson</title>
    	<meta name="viewport" content="width=device-width, initial-scale=1">
    	<link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet"> <!-- header font -->
		<link href="https://fonts.googleapis.com/css?family=Nanum+Gothic&display=swap" rel="stylesheet"> <!-- body font -->
		<link href="/base.css" rel="stylesheet" type="text/css"> <!-- base style -->

	</head>

	<style>

		#container{
			max-width:100%;
		}

		.row {
			display: table;
			width: 100%;
		}

		.column {
			display: table-cell;
			vertical-align: top;
			padding:20px;
		}

		.interests-card{
			padding-top:10px;
		}

		.card-header{
			font-family: 'Raleway', sans-serif !important;
			text-transform: uppercase;
			font-weight:bolder;
			border-top:2px solid black;
			padding:20px;
			width:100vw;
		}

		@media only screen and (max-width: 480px) {
		  	body{
				margin: 0px !important;
			}

		  	#container {
		  		width: 100vw !important;
		  	}
		}

	</style>

	<body>

		<div align="center">
			<div id="container">

				<div class="header">brett bankson</div>

				<div class = "nav" style="padding:20px;">
					<a href="/index">HOME</a>
					<a href="/about">ABOUT</a>
					<a href="/research">RESEARCH</a>
					<a href="/food">FOOD</a>
				</div>

				<div class = "nav" style="padding:10px 0px 0px 0px !important;border-top:2px solid black">
					<a href="#interests">RESEARCH INTERESTS</a>
					<a href="#papers">PAPERS</a>
				</div>

				<div id="interests" align="left">

					<div class="row" id="interests-header" style="font-size:2em;line-height:1.5em;padding:20px">
						The human brain rapidly and effortlessly extracts visual information from our environments on a millisecond-to-millisecond basis.
					</div> <!-- interests header -->

					<div class="row" id="interests-overview" style="padding:0px 20px 0px 20px">
						How does coordinated activity throughout visually responsive areas of the brain <b>compute this input to yield high-level information</b> that informs mental states and behavior? There are several research avenues I’m working on to further our understanding of this process, using spatiotemporally-resolved recordings from <i>intracranial electroencephalography (iEEG)</i> to provide sensitive and dynamic portraits of brain activity.
					</div> <!-- interests overview-->

					<div class="card-header">
						Spatiotemporal dynamics of face perception and natural vision
					</div>

					<div class="row" class="interests-card">
						<div class="column" style="width:50%;">
							When we see a face, we encode information from many feature dimensions – identity, age, gender, expression, gaze direction, etc. These features are functionally relevant and shape our interpretation of people in front of us. 
						</div>

						<div class="column" style="width:50%;">
							<ol>
								<li>
									How are different face feature dimensions that form and motion-related aspects encoded in time across ventral and dorsal visual streams?
								</li>
								<li>
									Do non-face preferring areas represent face information that is independent from canonical face preferring areas, and does this extend to other categories of visual objects?
								</li>
								<li>
									How does the brain extract identity and affective information from dynamic videos of spontaneous facial expressions?
								</li>
							</ol>
						</div>
					</div> <!--interests card -->


					<div class="card-header">
						Retinotopy and visual perception
					</div>

					<div class="row" class="interests-card">
						<div class="column" style="width:50%;">
							The location of an object in visual space drives neural activity selectively according to position near or far from fixation and along the horizontal and vertical meridians. The time course by which activity in category-selective areas is modulated by retinotopic variation is not well understood, and I’m working to better quantify the relationship between retinotopy and high-level visual processing.
						</div>

						<div class="column" style="width:50%;">
							<ol>
								<li>
									How does the position of a face in the left or right visual hemifield affect the representation of face identity in ventral temporal face selective areas?
								</li>
								<li>
									When does variation in retinotopic location at a fixed eccentricity affect representational fidelity for individual faces, words, and phase-scrambled stimuli?
								</li>
							</ol>
						</div>
					</div> <!--interests card -->

					<div class="row" class="interests-card">
						<div class="column" style="width:50%;">
							<span class="card-header">
								Numerical cognition and visual perception
							</span>
							<br>
							Most numerical reasoning that humans engage in relies on symbolic number characters (digits) to represent nonsymbolic magnitude information. It is currently unclear the extent to which magnitude representations are shared across representational formats (digits, words, dot arrays), and furthermore how these representations emerge in time. I'm currently working on these questions with collaborators at Pitt and Harvard.
						</div>

						<div class="column" style="width:50%;">
							<ol>
								<li>
									What is the time course by which individual numbers across various representational formats can be classified from whole-head MEG signal, and how do models of low-level visual features and magnitude explain this evoked response?
								</li>
								<li>
									How do number-selective sites in VTC and the inferior parietal sulcus (IPS) interact to assign magnitude information to visually-perceived digits and dot displays?
								</li>
							</ol>
						</div>
					</div> <!--interests card -->

				</div> <!-- interests -->

			</div> <!-- container -->
		</div>

	</body>

</html>